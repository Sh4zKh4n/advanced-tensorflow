{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# [Datasets Introduction for TensorFlow v.1.4.](# https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html)  \n",
    "\n",
    "I updated the code examples from a Sept. 2017 official Google/TensorFlow blog post on the Dataset API to v. 1.4 of TensorFlow.  \n",
    "\n",
    "cf. Blogpost [Introduction to TensorFlow Datasets and Estimators\n",
    "Tuesday, September 12, 2017\n",
    "](https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html), for the [Google Developers Blog](https://developers.googleblog.com/), https://goo.gl/Ujm2Ep  \n",
    "\n",
    "Also the original code for v. 1.3. in Python was [here](https://github.com/mhyttsten/Misc/blob/master/Blog_Estimators_DataSet.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.4.0\n"
     ]
    }
   ],
   "source": [
    "# Check that we have the correct TensorFlow version installed  \n",
    "tf_version = tf.__version__\n",
    "print(\"TensorFlow version: {}\".format(tf_version))\n",
    "assert \"1.3\" <= tf_version, \"TensorFlow r1.3 or later is needed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    }
   ],
   "source": [
    "# Windows users: You only need to change PATH, \n",
    "# otherwise I have changed this PATH to be a local relative path `./` \n",
    "# from the original, PATH = \"/tmp/tf_dataset_and_estimator_apis\"\n",
    "print(os.sep)\n",
    "PATH = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# `.csv` $\\to $ `tf.data.Dataset`\n",
    "\n",
    "Fetch and store Training and Test dataset files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset\n"
     ]
    }
   ],
   "source": [
    "PATH_DATASET = PATH + os.sep + \"dataset\"\n",
    "print(PATH_DATASET)\n",
    "FILE_TRAIN = PATH_DATASET + os.sep + \"iris_training.csv\"\n",
    "FILE_TEST  = PATH_DATASET + os.sep + \"iris_test.csv\"\n",
    "URL_TRAIN  = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "URL_TEST   = \"http://download.tensorflow.org/data/iris_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def downloadDataset(url, file):\n",
    "    if not os.path.exists(PATH_DATASET):\n",
    "        os.makedirs(PATH_DATASET)\n",
    "    if not os.path.exists(file):\n",
    "        data = urllib.urlopen(url).read()\n",
    "        with open(file,\"w\") as f:\n",
    "            f.write(data)\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "downloadDataset(URL_TRAIN,FILE_TRAIN)\n",
    "downloadDataset(URL_TEST,FILE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iris_training.csv', 'iris_test.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir( PATH_DATASET )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# tf.logging.set_verbosity(tf.logging.INFO)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The CSV features in our training & test data\n",
    "feature_names = [\n",
    "    'SepalLength',\n",
    "    'SepalWidth',\n",
    "    'PetalLength',\n",
    "    'PetalWidth'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Introducing the Datasets   \n",
    "\n",
    "cf. https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html  \n",
    "\n",
    "Datasets is a new way to create input pipelines to TensorFlow models.  This API is much more performant than using `feed_dict` or the queue-based pipelines, and it's cleaner and easier to use.  \n",
    "\n",
    "Although Datasets still resides in `tf.contrib.data` at 1.3, it's expected this API will move to core at 1.4, so it's high time to take it for a test drive.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "At a high-level, Datasets consists of the following classes and relations:\n",
    "\n",
    "$$\n",
    "\\text{TextLineDataset}, \\text{TFRecordDataset}, \\text{FixedLengthRecordDataset} \\in \\text{subclass} \\to \\text{Dataset} \\xrightarrow{\\text{instantiates}} \\text{Iterator}\n",
    "$$  \n",
    "\n",
    "where  \n",
    "* Dataset : Base class containing methods to create and transform datasets.  Also, allows you to initialize a dataset from data in memory, or from a Python generator  \n",
    "* TextLineDataset : Reads lines from text files.  \n",
    "* TFRecordDataset : Reads records from TFRecord files.\n",
    "* FixedLengthRecordDataset : Reads fixed size records from binary files. \n",
    "* Iterator : Provdes a way to access 1 dataset element at a time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create an input function, reading a file using the Dataset API.  \n",
    "Then provide the results to the Estimator API.  \n",
    "\n",
    "Arguments for this function are:\n",
    "* `file_path` : data file to read\n",
    "* `perform_shuffle` : whether record order should be randomized \n",
    "* `repeat_count` : number of times to iterate over records in the dataset.  e.g. if `repeat_count=1`, then each record is read once.  If `repeat_count=None`, iteration will continue forever.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def my_input_fn(file_path, perform_shuffle=False, repeat_count=1):\n",
    "    def decode_csv(line):\n",
    "        parsed_line = tf.decode_csv(line, [[0.],[0.,],[0.,],[0.,], [0]])\n",
    "        label = parsed_line[-1:] # Last element is the label\n",
    "        del parsed_line[-1] # Delete last element\n",
    "        features = parsed_line # Everything but last elements are the features \n",
    "        d = dict(zip(feature_names, features)), label\n",
    "        return d  \n",
    "    \n",
    "    dataset = (tf.data.TextLineDataset(file_path) # Read text file \n",
    "              .skip(1) # Skip header row\n",
    "              .map(decode_csv)) # Transform each elem by applying decode_csv fn\n",
    "    if perform_shuffle:\n",
    "        # Randomizes input using a window of 256 elements (read into memory)\n",
    "        dataset = dataset.shuffle(buffer_size=256)\n",
    "    dataset = dataset.repeat(repeat_count) # Repeats dataset this # times \n",
    "    dataset = dataset.batch(32) # Batch size to use \n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    batch_features, batch_labels= iterator.get_next()\n",
    "    return batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "When we train our model, we'll need a function that reads the input file and returns the feature and label data.  Estimators require that you create a function that'll have a return value of a tuple.  \n",
    "\n",
    "The return value must be a 2-element tuple organized as follows:\n",
    "* The 1st element must be a `dict` in which each input feature is a key, and then a list of values for the training batch.  \n",
    "* 2nd element is a list of labels for the training batch.  \n",
    "\n",
    "Since we're returning a batch of input features and training labels, it means that all lists in the return statement will have equal lengths.  Technically speaking, whenever we referred to \"list\" here, we actually mean a 1-dim. TensorFlow tensor.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note the following: \n",
    "* `TextLineDataset` : The Dataset API will do a lot of memory management for you when you're using its file-based datasets.  You can, for example, read in dataset files much larger than memory or read in multiple files by specifying a list as argument \n",
    "* `shuffle` : reads buffer_size records, then shuffles (randomizes) their order \n",
    "* `map` : calls the `decode_csv` function with each element in the dataset as an argument (since we're using `TextLineDataset`, each element will be a line of `.csv` text).  Then we apply `decode_csv` to each of the lines  \n",
    "* `decode_csv` : splits each line into fields, providing the default values if necessary.  Then returns a dict with the field keys and field values.  The map function updates each elem (line) in the dataset with the `dict`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "That's an introduction to Datasets!  For fun, use this function to print the first batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "next_batch = my_input_fn(FILE_TRAIN, True) # Will return 32 random elements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'tuple'>\n",
      "2\n",
      "<type 'dict'>\n",
      "['SepalLength', 'PetalWidth', 'PetalLength', 'SepalWidth']\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "(32,)\n",
      "(32,)\n",
      "(32,)\n",
      "(32,)\n",
      "<type 'numpy.ndarray'>\n",
      "(32, 1)\n",
      "({'SepalLength': array([ 6.30000019,  5.        ,  6.80000019,  6.4000001 ,  5.69999981,\n",
      "        6.30000019,  4.9000001 ,  6.69999981,  4.5999999 ,  5.69999981,\n",
      "        4.80000019,  5.19999981,  7.5999999 ,  6.69999981,  5.4000001 ,\n",
      "        6.9000001 ,  7.9000001 ,  6.        ,  4.4000001 ,  6.69999981,\n",
      "        5.        ,  6.4000001 ,  6.30000019,  5.69999981,  7.69999981,\n",
      "        4.9000001 ,  5.        ,  5.19999981,  5.0999999 ,  5.0999999 ,\n",
      "        7.        ,  5.        ], dtype=float32), 'PetalWidth': array([ 1.79999995,  0.2       ,  1.39999998,  2.20000005,  1.29999995,\n",
      "        1.29999995,  0.1       ,  2.4000001 ,  0.2       ,  0.40000001,\n",
      "        0.2       ,  0.2       ,  2.0999999 ,  2.29999995,  0.40000001,\n",
      "        1.5       ,  2.        ,  1.5       ,  0.2       ,  2.0999999 ,\n",
      "        0.2       ,  2.29999995,  1.60000002,  0.30000001,  2.29999995,\n",
      "        0.1       ,  0.60000002,  0.2       ,  0.30000001,  0.40000001,\n",
      "        1.39999998,  1.        ], dtype=float32), 'PetalLength': array([ 4.9000001 ,  1.39999998,  4.80000019,  5.5999999 ,  4.5       ,\n",
      "        4.4000001 ,  1.5       ,  5.5999999 ,  1.5       ,  1.5       ,\n",
      "        1.60000002,  1.39999998,  6.5999999 ,  5.19999981,  1.70000005,\n",
      "        4.9000001 ,  6.4000001 ,  5.        ,  1.29999995,  5.69999981,\n",
      "        1.5       ,  5.30000019,  4.69999981,  1.70000005,  6.9000001 ,\n",
      "        1.5       ,  1.60000002,  1.5       ,  1.39999998,  1.89999998,\n",
      "        4.69999981,  3.5       ], dtype=float32), 'SepalWidth': array([ 2.70000005,  3.5999999 ,  2.79999995,  2.79999995,  2.79999995,\n",
      "        2.29999995,  3.0999999 ,  3.0999999 ,  3.0999999 ,  4.4000001 ,\n",
      "        3.4000001 ,  3.4000001 ,  3.        ,  3.        ,  3.9000001 ,\n",
      "        3.0999999 ,  3.79999995,  2.20000005,  3.20000005,  3.29999995,\n",
      "        3.4000001 ,  3.20000005,  3.29999995,  3.79999995,  2.5999999 ,\n",
      "        3.0999999 ,  3.5       ,  3.5       ,  3.5       ,  3.79999995,\n",
      "        3.20000005,  2.        ], dtype=float32)}, array([[2],\n",
      "       [0],\n",
      "       [1],\n",
      "       [2],\n",
      "       [1],\n",
      "       [1],\n",
      "       [0],\n",
      "       [2],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [2],\n",
      "       [2],\n",
      "       [0],\n",
      "       [1],\n",
      "       [2],\n",
      "       [2],\n",
      "       [0],\n",
      "       [2],\n",
      "       [0],\n",
      "       [2],\n",
      "       [1],\n",
      "       [0],\n",
      "       [2],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [1],\n",
      "       [1]], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "# Now let's try it out, retrieving and printing 1 batch of data.\n",
    "# Although this code looks strange, you don't need to understand\n",
    "# the details\n",
    "with tf.Session() as sess:\n",
    "    first_batch = sess.run(next_batch)\n",
    "print(type(first_batch)) # tuple\n",
    "print(len(first_batch)) # 2\n",
    "print(type(first_batch[0])) # dict\n",
    "print(first_batch[0].keys())\n",
    "print(type(first_batch[0]['SepalLength'] )) # np.array\n",
    "print(type(first_batch[0][ feature_names[1]] )) # np.array\n",
    "print(type(first_batch[0][ feature_names[2]] )) # np.array\n",
    "print(type(first_batch[0][ feature_names[3]] )) # np.array\n",
    "print(first_batch[0]['SepalLength'].shape ) # np.array\n",
    "print(first_batch[0][ feature_names[1]].shape ) # np.array\n",
    "print(first_batch[0][ feature_names[2]].shape ) # np.array\n",
    "print(first_batch[0][ feature_names[3]].shape ) # np.array\n",
    "print(type(first_batch[1])) # np.array\n",
    "print(first_batch[1].shape) # (32,1)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Indeed, as a sanity check, we can look at the `.csv` files in `pandas`:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   120    4  setosa  versicolor  virginica\n",
      "0  6.4  2.8     5.6         2.2          2\n",
      "1  5.0  2.3     3.3         1.0          1\n",
      "2  4.9  2.5     4.5         1.7          2\n",
      "3  4.9  3.1     1.5         0.1          0\n",
      "4  5.7  3.8     1.7         0.3          0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>120</th>\n",
       "      <th>4</th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>120.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.845000</td>\n",
       "      <td>3.065000</td>\n",
       "      <td>3.739167</td>\n",
       "      <td>1.196667</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.868578</td>\n",
       "      <td>0.427156</td>\n",
       "      <td>1.822100</td>\n",
       "      <td>0.782039</td>\n",
       "      <td>0.840168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.400000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.075000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.425000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              120           4      setosa  versicolor   virginica\n",
       "count  120.000000  120.000000  120.000000  120.000000  120.000000\n",
       "mean     5.845000    3.065000    3.739167    1.196667    1.000000\n",
       "std      0.868578    0.427156    1.822100    0.782039    0.840168\n",
       "min      4.400000    2.000000    1.000000    0.100000    0.000000\n",
       "25%      5.075000    2.800000    1.500000    0.300000    0.000000\n",
       "50%      5.800000    3.000000    4.400000    1.300000    1.000000\n",
       "75%      6.425000    3.300000    5.100000    1.800000    2.000000\n",
       "max      7.900000    4.400000    6.900000    2.500000    2.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch_DF = pd.read_csv(FILE_TRAIN)\n",
    "print(first_batch_DF.head() )\n",
    "first_batch_DF.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The CSV features in our training & test data\n",
    "feature_names = [\n",
    "    'SepalLength',\n",
    "    'SepalWidth',\n",
    "    'PetalLength',\n",
    "    'PetalWidth'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def from_csv_to_ds_input_fn(file_path, m_i, feature_names, \n",
    "                            repeat_count=None, perform_shuffle=False, \n",
    "                               shuffle_buffer_size=4096):\n",
    "    \"\"\"\n",
    "    @fn from_csv_to_ds_input_fn\n",
    "    @param file_path\n",
    "    @param m_i, a positive integer, number of examples in a batch\n",
    "    @param feature_names, list of strings to name your d features \n",
    "    @param repeat_count, a positive integer or None, number of times to repeat, \n",
    "                None is for indefinitely\n",
    "    @param perform_shuffle = False , performs shuffling of data or not \n",
    "    @param shuffle_buffer_size = 4096, a positive integer\n",
    "    \"\"\"\n",
    "        \n",
    "    def decode_csv(line):\n",
    "        parsed_line = tf.decode_csv(line, record_defaults=[[0.],[0.,],[0.,],[0.,], [0]],\n",
    "                                        field_delim=',')\n",
    "        label = parsed_line[-1:] # Last element is the label\n",
    "        del parsed_line[-1] # Delete last element\n",
    "        features = parsed_line # Everything but last elements are the features \n",
    "\n",
    "        # X_i, y_i, only the last value in a line is the output value, y\n",
    "        # prior values, associated with a \"feature_name\", are input values  \n",
    "        d = dict(zip(feature_names, features)), label  \n",
    "        return d \n",
    "    \n",
    "    dataset = (tf.data.TextLineDataset(file_path) # Read text file \n",
    "                .skip(1) # Skip header row\n",
    "                .map(decode_csv, num_parallel_calls=m_i) # Transform each elem by applying decode_csv fn\n",
    "                .batch(m_i) # Batch size to use\n",
    "              )\n",
    "    if repeat_count is None:\n",
    "        dataset = dataset.repeat() # repeat indefinitely\n",
    "    else:\n",
    "        dataset = dataset.repeat(repeat_count) # Repeats dataset this # times \n",
    "\n",
    "    if perform_shuffle:\n",
    "        # Randomizes input using a window of shuffle_buffer_size elements (read into memory)\n",
    "        dataset = dataset.shuffle(buffer_size=shuffle_buffer_size)\n",
    "    \n",
    "    # create iterator\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "#    iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "    # Separate the input X data from the output y data\n",
    "    batch_features, batch_labels= iterator.get_next() \n",
    "\n",
    "    return batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Introducing Estimators  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create the feature_columns, which specifies the input to our model\n",
    "# All our input features are numeric, so use numeric_column for each one \n",
    "feature_columns = [tf.feature_column.numeric_column(k) for k in feature_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience (and education), let's use a `DNNClassifier`, and reiterate what the [API guide](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier) says:  \n",
    "\n",
    "### `tf.estimator.DNNClassifier`  \n",
    "\n",
    "Class `DNNClassifier`  \n",
    "\n",
    "#### `__init__`  \n",
    "```  \n",
    "__init__(\n",
    "    hidden_units, \n",
    "    feature_columns,\n",
    "    model_dir=None,\n",
    "    n_classes=2,\n",
    "    weight_column=None,\n",
    "    label_vocabulary=None, \n",
    "    optimizer='Adagrad', # an instance of `tf.Optimizer` used to train the model  \n",
    "    activation_fn=tf.nn.relu, \n",
    "    dropout=None, # When not None, probability we will drop out a given coordinate\n",
    "    input_layer_partitioner=None, # input_layer_partitioner: Optional.  Partitioner for input layer.  \n",
    "    config=None # RunConfig object configure the runtime settings\n",
    "```  \n",
    "\n",
    "For instance, for Args: \n",
    "\n",
    "`label_vocabulary`, - list of strings representing possible label values.  If given, labels must be string type and have any value in `label_vocabulary`.  If it's not given, that means labels are already encoded as inter or float within [0,1] for n_classes, and encoded as integer values in {0,1,...n_classses-1} for `n_classes` >2.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find a place to store our model  \n",
    "We'll have to create a new directory to store our model, graph, checkpoints; otherwise, we'll have to keep deleting all these files that are created.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DNNClassPATH = PATH + os.sep + 'DNNClass_iris'\n",
    "if not os.path.exists(DNNClassPATH):\n",
    "    os.makedirs(DNNClassPATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd3a4550a50>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': './DNNClass_iris', '_save_summary_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "# Create a deep neural network regression classifier \n",
    "# Use the DNNClassifier pre-made estimator  \n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns, # The input features to our model\n",
    "    hidden_units=[10,10], # Two layers, each with 10 neurons\n",
    "    n_classes=3,\n",
    "    model_dir=DNNClassPATH) # Path to where checkpoints etc. are stored  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    }
   ],
   "source": [
    "# originally\n",
    "#classifier.train(\n",
    "#    input_fn=lambda: my_input_fn(FILE_TRAIN,  True, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ./DNNClass_iris/model.ckpt.\n",
      "INFO:tensorflow:loss = 192.796, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 8 into ./DNNClass_iris/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 65.1756.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x7fd3a4550bd0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train our model, use the previous function from_csv_to_ds_input_fn  \n",
    "# Input to training is a file with training example\n",
    "# Stop training after 8 iterations of the training of the data (epochs)\n",
    "\n",
    "# Notice the parameter input_fn - we want the function that'll return our X_i, y_i, \n",
    "# NOT X_i, y_i themselves\n",
    "\n",
    "classifier.train(\n",
    "    input_fn=lambda: from_csv_to_ds_input_fn(FILE_TRAIN,\n",
    "                                                128, \n",
    "                                                feature_names,\n",
    "                                                 8, \n",
    "                                             True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`lambda: my_input_fn(FILE_TRAIN, True, 8)` is where we hook up Datasets with the Estimators!  \n",
    "\n",
    "Estimators need data to perform training, evaluation, and prediction, and it uses the `input_fn` to fetch the data.  \n",
    "\n",
    "Estimators require an `input_fn` with no arguments, so we create a function with no arguments using `lambda`, which calls `input_fn` with the desired arguments: `file_path`, `shuffle_setting`, `repeat_count`  \n",
    "\n",
    "In our case, we use our `my_input_fn`, passing it  \n",
    "* `FILE_TRAIN`, which is the training data file.  \n",
    "* `True`, which tells the Estimator to shuffle the data.\n",
    "* `8`, which tells the Estimator to and repeat the dataset 8 times.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Evaluating Our Trained Model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "How can we evaluate how well it's performing?  Fortunately, every Estimator contains an `evaluate` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2017-11-04-05:49:18\n",
      "INFO:tensorflow:Restoring parameters from ./DNNClass_iris/model.ckpt-8\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-04-05:49:18\n",
      "INFO:tensorflow:Saving dict for global step 8: accuracy = 0.8, average_loss = 0.574866, global_step = 8, loss = 17.246\n",
      "Evaluation results\n",
      "   average_loss, was: 0.574865758419\n",
      "   accuracy, was: 0.800000011921\n",
      "   global_step, was: 8\n",
      "   loss, was: 17.245973587\n"
     ]
    }
   ],
   "source": [
    "# original implementation\n",
    "\n",
    "# Evaluate our model using the examples contained in FILE_TEST\n",
    "# Return value will contain evaluation_metrics such as : loss & average_loss\n",
    "#evaluate_result = classifier.evaluate(\n",
    "#    input_fn=lambda: my_input_fn(FILE_TEST, False, 4))\n",
    "#print(\"Evaluation results\")\n",
    "#for key in evaluate_result:\n",
    "#    print(\"   {}, was: {}\".format(key, evaluate_result[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2017-11-04-05:49:36\n",
      "INFO:tensorflow:Restoring parameters from ./DNNClass_iris/model.ckpt-8\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-04-05:49:36\n",
      "INFO:tensorflow:Saving dict for global step 8: accuracy = 0.8, average_loss = 0.574866, global_step = 8, loss = 17.246\n",
      "Evaluation results\n",
      "   average_loss, was: 0.574865877628\n",
      "   accuracy, was: 0.800000011921\n",
      "   global_step, was: 8\n",
      "   loss, was: 17.2459754944\n"
     ]
    }
   ],
   "source": [
    "# Evaluate our model using the examples contained in FILE_TEST\n",
    "# Return value will contain evaluation_metrics such as : loss & average_loss\n",
    "evaluate_result = classifier.evaluate(\n",
    "    input_fn=lambda: from_csv_to_ds_input_fn(FILE_TEST,\n",
    "                                                128, \n",
    "                                                feature_names,\n",
    "                                                 4, \n",
    "                                             False))\n",
    "print(\"Evaluation results\")\n",
    "for key in evaluate_result:\n",
    "    print(\"   {}, was: {}\".format(key, evaluate_result[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we'd like to train our model further:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from ./DNNClass_iris/model.ckpt-8\n",
      "INFO:tensorflow:Saving checkpoints for 9 into ./DNNClass_iris/model.ckpt.\n",
      "INFO:tensorflow:loss = 62.9349, step = 9\n",
      "INFO:tensorflow:global_step/sec: 680.328\n",
      "INFO:tensorflow:loss = 16.9185, step = 109 (0.148 sec)\n",
      "INFO:tensorflow:global_step/sec: 669.819\n",
      "INFO:tensorflow:loss = 10.5667, step = 209 (0.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 766.319\n",
      "INFO:tensorflow:loss = 8.75861, step = 309 (0.130 sec)\n",
      "INFO:tensorflow:global_step/sec: 706.323\n",
      "INFO:tensorflow:loss = 7.76904, step = 409 (0.142 sec)\n",
      "INFO:tensorflow:global_step/sec: 654.528\n",
      "INFO:tensorflow:loss = 7.32616, step = 509 (0.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 652.018\n",
      "INFO:tensorflow:loss = 6.71862, step = 609 (0.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 573.217\n",
      "INFO:tensorflow:loss = 6.46066, step = 709 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 627.762\n",
      "INFO:tensorflow:loss = 6.14133, step = 809 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 550.322\n",
      "INFO:tensorflow:loss = 5.97194, step = 909 (0.182 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1008 into ./DNNClass_iris/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 5.75999.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x7fd3a4550bd0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.train(\n",
    "    input_fn=lambda: from_csv_to_ds_input_fn(FILE_TRAIN,\n",
    "                                                128, \n",
    "                                                feature_names,\n",
    "                                                 1000, \n",
    "                                             True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2017-11-04-05:50:09\n",
      "INFO:tensorflow:Restoring parameters from ./DNNClass_iris/model.ckpt-1008\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-04-05:50:09\n",
      "INFO:tensorflow:Saving dict for global step 1008: accuracy = 0.966667, average_loss = 0.0595646, global_step = 1008, loss = 1.78694\n",
      "Evaluation results\n",
      "   average_loss, was: 0.0595646351576\n",
      "   accuracy, was: 0.966666638851\n",
      "   global_step, was: 1008\n",
      "   loss, was: 1.78693902493\n"
     ]
    }
   ],
   "source": [
    "evaluate_result = classifier.evaluate(\n",
    "    input_fn=lambda: from_csv_to_ds_input_fn(FILE_TEST,\n",
    "                                                128, \n",
    "                                                feature_names,\n",
    "                                                 4, \n",
    "                                             False))\n",
    "print(\"Evaluation results\")\n",
    "for key in evaluate_result:\n",
    "    print(\"   {}, was: {}\".format(key, evaluate_result[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Making Predictions Using Our Trained Model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on test file\n"
     ]
    }
   ],
   "source": [
    "# originally\n",
    "# Predict the type of some Iris flowers. \n",
    "# Let's predict the examples in FILE_TEST, repeat only once.  \n",
    "#predict_results = classifier.predict(\n",
    "#    input_fn=lambda: my_input_fn(FILE_TEST,False,1))\n",
    "#print(\"Predictions on test file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on test file\n"
     ]
    }
   ],
   "source": [
    "predict_results = classifier.predict(\n",
    "    input_fn=lambda: from_csv_to_ds_input_fn(FILE_TEST,\n",
    "                                                128, \n",
    "                                                feature_names,\n",
    "                                                 1, \n",
    "                                             False))\n",
    "print(\"Predictions on test file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Input graph does not contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Restoring parameters from ./model.ckpt-1008\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for prediction in predict_results:\n",
    "    # Will print the predicted class, i.e.: 0, ,1, or 2 if the prediction \n",
    "    # is Iris Sentosa, Vericolor, Virginica, respectively.  \n",
    "    print(prediction[\"class_ids\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Making Predictions on Data in Memory  \n",
    "\n",
    "How could we make predictions on data residing in other sources, for example, in memory?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Let create a memory dataset for prediction.  \n",
    "# We've taken the first 3 examples in FILE_TEST.  \n",
    "prediction_input = [[5.9, 3.0, 4.2, 1.5], # -> 1, Iris Versicolor \n",
    "                    [6.9, 3.1, 5.4, 2.1], # -> 2, Iris Virginica\n",
    "                    [5.1, 3.3, 1.7, 0.5]] # -> 0, Iris Sentosa \n",
    "\n",
    "def new_input_fn():\n",
    "    def decode(x):\n",
    "        x = tf.split(x, 4) # Need to split into our 4 features\n",
    "        # When predicting, we don't need (or have) any labels\n",
    "        return dict(zip(feature_names, x)) # To build a dict of them\n",
    "    \n",
    "    # The from_tensor_slices function will use a memory structure as input\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(prediction_input)\n",
    "    dataset = dataset.map(decode)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    next_feature_batch = iterator.get_next()\n",
    "    return next_feature_batch, None # In prediction, we have no labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Predict all our prediction_input\n",
    "predict_results = classifier.predict(input_fn=new_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on memory data\n",
      "WARNING:tensorflow:From <ipython-input-22-ee04ea1022d7>:14: from_tensor_slices (from tensorflow.contrib.data.python.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.from_tensor_slices()`.\n",
      "WARNING:tensorflow:Input graph does not contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Restoring parameters from ./model.ckpt-1008\n",
      "I think: [5.9, 3.0, 4.2, 1.5], is Iris Versicolor\n",
      "I think: [6.9, 3.1, 5.4, 2.1], is Iris Virginica\n",
      "I think: [5.1, 3.3, 1.7, 0.5], is Iris Sentosa\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(\"Predictions on memory data\")\n",
    "for idx, prediction in enumerate(predict_results):\n",
    "    type = prediction[\"class_ids\"][0] # Get the predicted class (index)\n",
    "    if type == 0:\n",
    "        print(\"I think: {}, is Iris Sentosa\".format(prediction_input[idx]))\n",
    "    elif type == 1:\n",
    "        print(\"I think: {}, is Iris Versicolor\".format(prediction_input[idx]))\n",
    "    else:\n",
    "        print(\"I think: {}, is Iris Virginica\".format(prediction_input[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./DNNClass_iris'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can go use TensorBoard\n",
    "# in the command line\n",
    "# tensorboard --logdir=PATH\n",
    "# where PATH is DNNClassPATH in this case\n",
    "# make sure to be in virtual-env\n",
    "DNNClassPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Other estimators; LinearClassifier, DNNLinearCombinedClassifier\n",
    "\n",
    "#### `tf.estimator.LinearClassifer`  \n",
    "\n",
    "Class `LinearClassifier`  \n",
    "\n",
    "`__init__`  \n",
    "\n",
    "```  \n",
    "__init__(\n",
    "    feature_columns, \n",
    "    model_dir=None, \n",
    "    n_classes=2, \n",
    "    weight_column=None,\n",
    "    label_vocabulary=None,\n",
    "    optimizer='Ftrl', \n",
    "    config=None,\n",
    "    partitioner=None\n",
    ")  \n",
    "```  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[_NumericColumn(key='SepalLength', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='SepalWidth', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='PetalLength', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='PetalWidth', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LinClassPATH = PATH + os.sep + 'LinClass_iris'\n",
    "if not os.path.exists(LinClassPATH):\n",
    "    os.makedirs(LinClassPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd3a44ecbd0>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': './LinClass_iris', '_save_summary_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "Linclassifier = tf.estimator.LinearClassifier(\n",
    "    feature_columns=feature_columns, # The input features to our model\n",
    "    n_classes=3,\n",
    "    model_dir=LinClassPATH) # Path to where checkpoints etc. are stored  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run into trouble here at this point, you'll have to manually go and delete the graphs, the checkpoints, and start over, or change `model_dir`, accordingly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ./LinClass_iris/model.ckpt.\n",
      "INFO:tensorflow:loss = 131.833, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 8 into ./LinClass_iris/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 80.3571.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.linear.LinearClassifier at 0x7fd3a44ece90>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Linclassifier.train(\n",
    "    input_fn=lambda: from_csv_to_ds_input_fn(FILE_TRAIN,\n",
    "                                                128, \n",
    "                                                feature_names,\n",
    "                                                 8, \n",
    "                                             True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2017-11-04-05:53:06\n",
      "INFO:tensorflow:Restoring parameters from ./LinClass_iris/model.ckpt-8\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-04-05:53:07\n",
      "INFO:tensorflow:Saving dict for global step 8: accuracy = 0.733333, average_loss = 0.67093, global_step = 8, loss = 20.1279\n",
      "Evaluation results\n",
      "   average_loss, was: 0.670929729939\n",
      "   accuracy, was: 0.733333349228\n",
      "   global_step, was: 8\n",
      "   loss, was: 20.1278915405\n"
     ]
    }
   ],
   "source": [
    "evaluate_result = Linclassifier.evaluate(\n",
    "    input_fn=lambda: from_csv_to_ds_input_fn(FILE_TEST,\n",
    "                                                128, \n",
    "                                                feature_names,\n",
    "                                                 4, \n",
    "                                             False))\n",
    "print(\"Evaluation results\")\n",
    "for key in evaluate_result:\n",
    "    print(\"   {}, was: {}\".format(key, evaluate_result[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we'd like to train further, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from ./LinClass_iris/model.ckpt-8\n",
      "INFO:tensorflow:Saving checkpoints for 9 into ./LinClass_iris/model.ckpt.\n",
      "INFO:tensorflow:loss = 76.4313, step = 9\n",
      "INFO:tensorflow:global_step/sec: 519.646\n",
      "INFO:tensorflow:loss = 36.0779, step = 109 (0.193 sec)\n",
      "INFO:tensorflow:global_step/sec: 761.858\n",
      "INFO:tensorflow:loss = 27.3667, step = 209 (0.131 sec)\n",
      "INFO:tensorflow:global_step/sec: 826.194\n",
      "INFO:tensorflow:loss = 22.7559, step = 309 (0.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 844.759\n",
      "INFO:tensorflow:loss = 19.8666, step = 409 (0.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 800.698\n",
      "INFO:tensorflow:loss = 17.8713, step = 509 (0.125 sec)\n",
      "INFO:tensorflow:global_step/sec: 701.843\n",
      "INFO:tensorflow:loss = 16.4017, step = 609 (0.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 558.791\n",
      "INFO:tensorflow:loss = 15.2685, step = 709 (0.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 610.422\n",
      "INFO:tensorflow:loss = 14.3644, step = 809 (0.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 655.304\n",
      "INFO:tensorflow:loss = 13.6239, step = 909 (0.153 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1008 into ./LinClass_iris/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 13.0102.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.linear.LinearClassifier at 0x7fd3a44ece90>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Linclassifier.train(\n",
    "    input_fn=lambda: from_csv_to_ds_input_fn(FILE_TRAIN,\n",
    "                                                128, \n",
    "                                                feature_names,\n",
    "                                                 1000, \n",
    "                                             True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2017-11-04-05:54:29\n",
      "INFO:tensorflow:Restoring parameters from ./LinClass_iris/model.ckpt-1008\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-04-05:54:30\n",
      "INFO:tensorflow:Saving dict for global step 1008: accuracy = 0.966667, average_loss = 0.120493, global_step = 1008, loss = 3.6148\n",
      "Evaluation results\n",
      "   average_loss, was: 0.120493300259\n",
      "   accuracy, was: 0.966666638851\n",
      "   global_step, was: 1008\n",
      "   loss, was: 3.61479902267\n"
     ]
    }
   ],
   "source": [
    "evaluate_result = Linclassifier.evaluate(\n",
    "    input_fn=lambda: from_csv_to_ds_input_fn(FILE_TEST,\n",
    "                                                128, \n",
    "                                                feature_names,\n",
    "                                                 4, \n",
    "                                             False))\n",
    "print(\"Evaluation results\")\n",
    "for key in evaluate_result:\n",
    "    print(\"   {}, was: {}\".format(key, evaluate_result[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./LinClass_iris'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can go use TensorBoard\n",
    "# in the command line\n",
    "# tensorboard --logdir=PATH\n",
    "# where PATH is DNNClassPATH in this case\n",
    "# make sure to be in virtual-env\n",
    "LinClassPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `tf.estimator.DNNLinearCombinedClassifer`  \n",
    "\n",
    "Class `DNNLinearCombinedClassifier`  \n",
    "\n",
    "`__init__`  \n",
    "\n",
    "```  \n",
    "__init__(\n",
    "    model_dir=None, \n",
    "    linear_feature_columns=None,\n",
    "    linear_optimizer='Ftrl'\n",
    "    dnn_feature_columns=None,\n",
    "    dnn_optimizer='Adagrad',\n",
    "    dnn_hidden_units=None,\n",
    "    dnn_activation_fn=tf.nn.relu,\n",
    "    dnn_dropout=None,\n",
    "    n_classes=3, \n",
    "    weight_column=None,\n",
    "    label_vocabulary=None,\n",
    "    input_layer_partitioner=None,\n",
    "    config=None,\n",
    ")  \n",
    "```  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DNNLinClassPATH = PATH + os.sep + 'DNNLinClass_iris'\n",
    "if not os.path.exists(DNNLinClassPATH):\n",
    "    os.makedirs(DNNLinClassPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd3a450e2d0>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': './DNNLinClass_iris', '_save_summary_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "DNNLinclassifier = tf.estimator.DNNLinearCombinedClassifier(\n",
    "    dnn_feature_columns=feature_columns, # The input features to our model\n",
    "    dnn_hidden_units=[8,16,8,4],\n",
    "    n_classes=3,\n",
    "    model_dir=DNNLinClassPATH) # Path to where checkpoints etc. are stored  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from ./DNNLinClass_iris/model.ckpt-1000\n",
      "INFO:tensorflow:Saving checkpoints for 1001 into ./DNNLinClass_iris/model.ckpt.\n",
      "INFO:tensorflow:loss = 122.452, step = 1001\n",
      "INFO:tensorflow:global_step/sec: 328.806\n",
      "INFO:tensorflow:loss = 120.885, step = 1101 (0.306 sec)\n",
      "INFO:tensorflow:global_step/sec: 352.901\n",
      "INFO:tensorflow:loss = 119.035, step = 1201 (0.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 358.743\n",
      "INFO:tensorflow:loss = 117.388, step = 1301 (0.273 sec)\n",
      "INFO:tensorflow:global_step/sec: 351.008\n",
      "INFO:tensorflow:loss = 115.876, step = 1401 (0.288 sec)\n",
      "INFO:tensorflow:global_step/sec: 305.584\n",
      "INFO:tensorflow:loss = 114.455, step = 1501 (0.324 sec)\n",
      "INFO:tensorflow:global_step/sec: 306.192\n",
      "INFO:tensorflow:loss = 113.115, step = 1601 (0.327 sec)\n",
      "INFO:tensorflow:global_step/sec: 362.636\n",
      "INFO:tensorflow:loss = 111.813, step = 1701 (0.276 sec)\n",
      "INFO:tensorflow:global_step/sec: 376.261\n",
      "INFO:tensorflow:loss = 110.536, step = 1801 (0.265 sec)\n",
      "INFO:tensorflow:global_step/sec: 365.045\n",
      "INFO:tensorflow:loss = 109.286, step = 1901 (0.275 sec)\n",
      "INFO:tensorflow:global_step/sec: 348.262\n",
      "INFO:tensorflow:loss = 108.06, step = 2001 (0.286 sec)\n",
      "INFO:tensorflow:global_step/sec: 339.144\n",
      "INFO:tensorflow:loss = 106.882, step = 2101 (0.296 sec)\n",
      "INFO:tensorflow:global_step/sec: 355.509\n",
      "INFO:tensorflow:loss = 105.741, step = 2201 (0.287 sec)\n",
      "INFO:tensorflow:global_step/sec: 284.129\n",
      "INFO:tensorflow:loss = 104.625, step = 2301 (0.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 348.097\n",
      "INFO:tensorflow:loss = 103.523, step = 2401 (0.284 sec)\n",
      "INFO:tensorflow:global_step/sec: 379.816\n",
      "INFO:tensorflow:loss = 102.448, step = 2501 (0.264 sec)\n",
      "INFO:tensorflow:global_step/sec: 297.785\n",
      "INFO:tensorflow:loss = 101.393, step = 2601 (0.343 sec)\n",
      "INFO:tensorflow:global_step/sec: 350.533\n",
      "INFO:tensorflow:loss = 100.35, step = 2701 (0.277 sec)\n",
      "INFO:tensorflow:global_step/sec: 363.523\n",
      "INFO:tensorflow:loss = 99.3185, step = 2801 (0.275 sec)\n",
      "INFO:tensorflow:global_step/sec: 362.444\n",
      "INFO:tensorflow:loss = 98.3064, step = 2901 (0.276 sec)\n",
      "INFO:tensorflow:global_step/sec: 298.251\n",
      "INFO:tensorflow:loss = 97.309, step = 3001 (0.335 sec)\n",
      "INFO:tensorflow:global_step/sec: 374.054\n",
      "INFO:tensorflow:loss = 96.3303, step = 3101 (0.267 sec)\n",
      "INFO:tensorflow:global_step/sec: 360.276\n",
      "INFO:tensorflow:loss = 95.3688, step = 3201 (0.277 sec)\n",
      "INFO:tensorflow:global_step/sec: 296.754\n",
      "INFO:tensorflow:loss = 94.4175, step = 3301 (0.337 sec)\n",
      "INFO:tensorflow:global_step/sec: 364.6\n",
      "INFO:tensorflow:loss = 93.4765, step = 3401 (0.274 sec)\n",
      "INFO:tensorflow:global_step/sec: 342.783\n",
      "INFO:tensorflow:loss = 92.5522, step = 3501 (0.293 sec)\n",
      "INFO:tensorflow:global_step/sec: 376.729\n",
      "INFO:tensorflow:loss = 91.6418, step = 3601 (0.272 sec)\n",
      "INFO:tensorflow:global_step/sec: 351.016\n",
      "INFO:tensorflow:loss = 90.745, step = 3701 (0.278 sec)\n",
      "INFO:tensorflow:global_step/sec: 357.079\n",
      "INFO:tensorflow:loss = 89.8602, step = 3801 (0.280 sec)\n",
      "INFO:tensorflow:global_step/sec: 350.723\n",
      "INFO:tensorflow:loss = 88.9864, step = 3901 (0.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 280.873\n",
      "INFO:tensorflow:loss = 88.1216, step = 4001 (0.351 sec)\n",
      "INFO:tensorflow:global_step/sec: 357.9\n",
      "INFO:tensorflow:loss = 87.2653, step = 4101 (0.279 sec)\n",
      "INFO:tensorflow:global_step/sec: 351.409\n",
      "INFO:tensorflow:loss = 86.4181, step = 4201 (0.285 sec)\n",
      "INFO:tensorflow:global_step/sec: 337.087\n",
      "INFO:tensorflow:loss = 85.5799, step = 4301 (0.297 sec)\n",
      "INFO:tensorflow:global_step/sec: 321.588\n",
      "INFO:tensorflow:loss = 84.7505, step = 4401 (0.311 sec)\n",
      "INFO:tensorflow:global_step/sec: 343.788\n",
      "INFO:tensorflow:loss = 83.9304, step = 4501 (0.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 362.704\n",
      "INFO:tensorflow:loss = 83.1203, step = 4601 (0.276 sec)\n",
      "INFO:tensorflow:global_step/sec: 323.852\n",
      "INFO:tensorflow:loss = 82.3215, step = 4701 (0.309 sec)\n",
      "INFO:tensorflow:global_step/sec: 370.161\n",
      "INFO:tensorflow:loss = 81.5332, step = 4801 (0.271 sec)\n",
      "INFO:tensorflow:global_step/sec: 314.596\n",
      "INFO:tensorflow:loss = 80.7564, step = 4901 (0.317 sec)\n",
      "INFO:tensorflow:global_step/sec: 334.678\n",
      "INFO:tensorflow:loss = 79.9906, step = 5001 (0.299 sec)\n",
      "INFO:tensorflow:global_step/sec: 371.07\n",
      "INFO:tensorflow:loss = 79.2367, step = 5101 (0.270 sec)\n",
      "INFO:tensorflow:global_step/sec: 343.501\n",
      "INFO:tensorflow:loss = 78.4945, step = 5201 (0.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 363.941\n",
      "INFO:tensorflow:loss = 77.7642, step = 5301 (0.279 sec)\n",
      "INFO:tensorflow:global_step/sec: 344.8\n",
      "INFO:tensorflow:loss = 77.0461, step = 5401 (0.287 sec)\n",
      "INFO:tensorflow:global_step/sec: 354.405\n",
      "INFO:tensorflow:loss = 76.344, step = 5501 (0.282 sec)\n",
      "INFO:tensorflow:global_step/sec: 314.616\n",
      "INFO:tensorflow:loss = 75.6561, step = 5601 (0.322 sec)\n",
      "INFO:tensorflow:global_step/sec: 365.837\n",
      "INFO:tensorflow:loss = 74.9814, step = 5701 (0.273 sec)\n",
      "INFO:tensorflow:global_step/sec: 360.235\n",
      "INFO:tensorflow:loss = 74.3195, step = 5801 (0.278 sec)\n",
      "INFO:tensorflow:global_step/sec: 365.156\n",
      "INFO:tensorflow:loss = 73.6703, step = 5901 (0.270 sec)\n",
      "INFO:tensorflow:global_step/sec: 357.614\n",
      "INFO:tensorflow:loss = 73.0334, step = 6001 (0.280 sec)\n",
      "INFO:tensorflow:global_step/sec: 351.528\n",
      "INFO:tensorflow:loss = 72.409, step = 6101 (0.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 333.37\n",
      "INFO:tensorflow:loss = 71.7971, step = 6201 (0.296 sec)\n",
      "INFO:tensorflow:global_step/sec: 344.917\n",
      "INFO:tensorflow:loss = 71.1986, step = 6301 (0.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 371.123\n",
      "INFO:tensorflow:loss = 70.6136, step = 6401 (0.274 sec)\n",
      "INFO:tensorflow:global_step/sec: 294.165\n",
      "INFO:tensorflow:loss = 70.0413, step = 6501 (0.335 sec)\n",
      "INFO:tensorflow:global_step/sec: 305.928\n",
      "INFO:tensorflow:loss = 69.4813, step = 6601 (0.333 sec)\n",
      "INFO:tensorflow:global_step/sec: 302.138\n",
      "INFO:tensorflow:loss = 68.9333, step = 6701 (0.326 sec)\n",
      "INFO:tensorflow:global_step/sec: 322.955\n",
      "INFO:tensorflow:loss = 68.3973, step = 6801 (0.309 sec)\n",
      "INFO:tensorflow:global_step/sec: 322.108\n",
      "INFO:tensorflow:loss = 67.8729, step = 6901 (0.312 sec)\n",
      "INFO:tensorflow:global_step/sec: 597.04\n",
      "INFO:tensorflow:loss = 67.3627, step = 7001 (0.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 587.527\n",
      "INFO:tensorflow:loss = 66.8653, step = 7101 (0.170 sec)\n",
      "INFO:tensorflow:global_step/sec: 557.165\n",
      "INFO:tensorflow:loss = 66.3803, step = 7201 (0.180 sec)\n",
      "INFO:tensorflow:global_step/sec: 626.684\n",
      "INFO:tensorflow:loss = 65.907, step = 7301 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 585.175\n",
      "INFO:tensorflow:loss = 65.445, step = 7401 (0.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 640.989\n",
      "INFO:tensorflow:loss = 64.9939, step = 7501 (0.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 640.964\n",
      "INFO:tensorflow:loss = 64.5535, step = 7601 (0.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 660.118\n",
      "INFO:tensorflow:loss = 64.1235, step = 7701 (0.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 544.618\n",
      "INFO:tensorflow:loss = 63.7036, step = 7801 (0.183 sec)\n",
      "INFO:tensorflow:global_step/sec: 670.039\n",
      "INFO:tensorflow:loss = 63.2934, step = 7901 (0.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 641.244\n",
      "INFO:tensorflow:loss = 62.8932, step = 8001 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 622.169\n",
      "INFO:tensorflow:loss = 62.5039, step = 8101 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 753.823\n",
      "INFO:tensorflow:loss = 62.1245, step = 8201 (0.133 sec)\n",
      "INFO:tensorflow:global_step/sec: 684.735\n",
      "INFO:tensorflow:loss = 61.7545, step = 8301 (0.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 710.287\n",
      "INFO:tensorflow:loss = 61.3935, step = 8401 (0.141 sec)\n",
      "INFO:tensorflow:global_step/sec: 721.241\n",
      "INFO:tensorflow:loss = 61.0413, step = 8501 (0.139 sec)\n",
      "INFO:tensorflow:global_step/sec: 749.45\n",
      "INFO:tensorflow:loss = 60.6975, step = 8601 (0.134 sec)\n",
      "INFO:tensorflow:global_step/sec: 780.933\n",
      "INFO:tensorflow:loss = 60.3635, step = 8701 (0.127 sec)\n",
      "INFO:tensorflow:global_step/sec: 796.489\n",
      "INFO:tensorflow:loss = 60.0388, step = 8801 (0.126 sec)\n",
      "INFO:tensorflow:global_step/sec: 773.36\n",
      "INFO:tensorflow:loss = 59.7229, step = 8901 (0.130 sec)\n",
      "INFO:tensorflow:global_step/sec: 621.937\n",
      "INFO:tensorflow:loss = 59.4155, step = 9001 (0.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 710.126\n",
      "INFO:tensorflow:loss = 59.1163, step = 9101 (0.141 sec)\n",
      "INFO:tensorflow:global_step/sec: 591.017\n",
      "INFO:tensorflow:loss = 58.8249, step = 9201 (0.170 sec)\n",
      "INFO:tensorflow:global_step/sec: 551.043\n",
      "INFO:tensorflow:loss = 58.541, step = 9301 (0.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 601.514\n",
      "INFO:tensorflow:loss = 58.2644, step = 9401 (0.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 558.164\n",
      "INFO:tensorflow:loss = 57.9949, step = 9501 (0.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 602.769\n",
      "INFO:tensorflow:loss = 57.7322, step = 9601 (0.169 sec)\n",
      "INFO:tensorflow:global_step/sec: 606.572\n",
      "INFO:tensorflow:loss = 57.4762, step = 9701 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 616.31\n",
      "INFO:tensorflow:loss = 57.2266, step = 9801 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 663.103\n",
      "INFO:tensorflow:loss = 56.9831, step = 9901 (0.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 666.129\n",
      "INFO:tensorflow:loss = 56.7459, step = 10001 (0.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 717.757\n",
      "INFO:tensorflow:loss = 56.5146, step = 10101 (0.140 sec)\n",
      "INFO:tensorflow:global_step/sec: 613.444\n",
      "INFO:tensorflow:loss = 56.2891, step = 10201 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 727.214\n",
      "INFO:tensorflow:loss = 56.0692, step = 10301 (0.139 sec)\n",
      "INFO:tensorflow:global_step/sec: 579.273\n",
      "INFO:tensorflow:loss = 55.8546, step = 10401 (0.172 sec)\n",
      "INFO:tensorflow:global_step/sec: 658.384\n",
      "INFO:tensorflow:loss = 55.6452, step = 10501 (0.151 sec)\n",
      "INFO:tensorflow:global_step/sec: 711.937\n",
      "INFO:tensorflow:loss = 55.4409, step = 10601 (0.142 sec)\n",
      "INFO:tensorflow:global_step/sec: 575.347\n",
      "INFO:tensorflow:loss = 55.2414, step = 10701 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 641.824\n",
      "INFO:tensorflow:loss = 55.0466, step = 10801 (0.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 658.177\n",
      "INFO:tensorflow:loss = 54.8564, step = 10901 (0.152 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 11000 into ./DNNLinClass_iris/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 54.6726.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn_linear_combined.DNNLinearCombinedClassifier at 0x7fd3edc7f090>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DNNLinclassifier.train(\n",
    "    input_fn=lambda: from_csv_to_ds_input_fn(FILE_TRAIN,\n",
    "                                                128, \n",
    "                                                feature_names,\n",
    "                                                 10000, \n",
    "                                             True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2017-11-04-07:36:24\n",
      "INFO:tensorflow:Restoring parameters from ./DNNLinClass_iris/model.ckpt-11000\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-04-07:36:24\n",
      "INFO:tensorflow:Saving dict for global step 11000: accuracy = 0.533333, average_loss = 0.577451, global_step = 11000, loss = 17.3235\n",
      "Evaluation results\n",
      "   average_loss, was: 0.577450931072\n",
      "   accuracy, was: 0.533333361149\n",
      "   global_step, was: 11000\n",
      "   loss, was: 17.3235282898\n"
     ]
    }
   ],
   "source": [
    "evaluate_result = DNNLinclassifier.evaluate(\n",
    "    input_fn=lambda: from_csv_to_ds_input_fn(FILE_TEST,\n",
    "                                                128, \n",
    "                                                feature_names,\n",
    "                                                 4, \n",
    "                                             False))\n",
    "print(\"Evaluation results\")\n",
    "for key in evaluate_result:\n",
    "    print(\"   {}, was: {}\".format(key, evaluate_result[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
